# -*- coding: utf-8 -*-
"""
ì™„ë²½í•œ GPT-3 ì±—ë´‡ ì„œë²„
Flask + PyTorch íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from flask import Flask, request, jsonify
from flask_cors import CORS
import json
import os

# ========== GPT-3 ëª¨ë¸ ì •ì˜ ==========

class CausalSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, block_size):
        super().__init__()
        assert embed_dim % num_heads == 0
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)
        self.out = nn.Linear(embed_dim, embed_dim)
        self.register_buffer(
            "mask",
            torch.tril(torch.ones(block_size, block_size))
            .view(1, 1, block_size, block_size)
        )

    def forward(self, x):
        B, T, C = x.size()
        qkv = self.qkv(x)
        q, k, v = qkv.chunk(3, dim=-1)
        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        att = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float("-inf"))
        att = F.softmax(att, dim=-1)
        out = att @ v
        out = out.transpose(1, 2).contiguous().view(B, T, C)
        return self.out(out)

class MLP(nn.Module):
    def __init__(self, embed_dim):
        super().__init__()
        self.fc1 = nn.Linear(embed_dim, 4 * embed_dim)
        self.fc2 = nn.Linear(4 * embed_dim, embed_dim)

    def forward(self, x):
        return self.fc2(F.gelu(self.fc1(x)))

class TransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, block_size):
        super().__init__()
        self.ln1 = nn.LayerNorm(embed_dim)
        self.attn = CausalSelfAttention(embed_dim, num_heads, block_size)
        self.ln2 = nn.LayerNorm(embed_dim)
        self.mlp = MLP(embed_dim)

    def forward(self, x):
        x = x + self.attn(self.ln1(x))
        x = x + self.mlp(self.ln2(x))
        return x

class GPT3(nn.Module):
    def __init__(self, vocab_size, block_size, n_layers=6, embed_dim=256, n_heads=8):
        super().__init__()
        self.token_emb = nn.Embedding(vocab_size, embed_dim)
        self.pos_emb = nn.Embedding(block_size, embed_dim)
        self.blocks = nn.Sequential(
            *[TransformerBlock(embed_dim, n_heads, block_size) for _ in range(n_layers)]
        )
        self.ln_f = nn.LayerNorm(embed_dim)
        self.head = nn.Linear(embed_dim, vocab_size, bias=False)
        self.block_size = block_size

    def forward(self, idx, targets=None):
        B, T = idx.size()
        pos = torch.arange(0, T, device=idx.device)
        x = self.token_emb(idx) + self.pos_emb(pos)
        x = self.blocks(x)
        x = self.ln_f(x)
        logits = self.head(x)
        loss = None
        if targets is not None:
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))
        return logits, loss

@torch.no_grad()
def generate(model, idx, max_new_tokens, temperature=1.0):
    model.eval()
    for _ in range(max_new_tokens):
        idx_cond = idx if idx.size(1) <= model.block_size else idx[:, -model.block_size:]
        logits, _ = model(idx_cond)
        logits = logits[:, -1, :] / temperature
        probs = F.softmax(logits, dim=-1)
        idx_next = torch.multinomial(probs, 1)
        idx = torch.cat([idx, idx_next], dim=1)
    return idx

# ========== í† í¬ë‚˜ì´ì € ==========

class SimpleTokenizer:
    def __init__(self):
        # í•œê¸€ + ì˜ì–´ + ìˆ«ì + íŠ¹ìˆ˜ë¬¸ì ì§€ì›
        self.chars = sorted(list(set(
            "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 "
            "ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨ì¹´íƒ€íŒŒí•˜ê±°ë„ˆë”ëŸ¬ë¨¸ë²„ì„œì–´ì €ì²˜ì»¤í„°í¼í—ˆ"
            "ê³ ë…¸ë„ë¡œëª¨ë³´ì†Œì˜¤ì¡°ì´ˆì½”í† í¬í˜¸êµ¬ëˆ„ë‘ë£¨ë¬´ë¶€ìˆ˜ìš°ì£¼ì¶”ì¿ íˆ¬í‘¸í›„"
            "ê·¸ëŠë“œë¥´ë¯€ë¸ŒìŠ¤ìœ¼ì¦ˆì¸ í¬íŠ¸í”„íê¸°ë‹ˆë””ë¦¬ë¯¸ë¹„ì‹œì´ì§€ì¹˜í‚¤í‹°í”¼íˆ"
            "!@#$%^&*()_+-=[]{}|;':\",./<>?~`"
            "ì•ˆë…•í•˜ì„¸ìš”ê°ì‚¬í•©ë‹ˆë‹¤ì¸ê³µì§€ëŠ¥ì±—ë´‡ë™ì•„ë¦¬í•™ìŠµìƒì„±í˜•ìµœê³ "
        )))
        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}
        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}
        self.vocab_size = len(self.chars)

    def encode(self, text):
        return [self.char_to_idx.get(c, 0) for c in text]

    def decode(self, indices):
        return ''.join([self.idx_to_char.get(i, '') for i in indices])

# ========== í•™ìŠµ ë°ì´í„° ==========

TRAINING_DATA = """
ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤!
ì±—ë´‡ì´ ë­ì˜ˆìš”? ì±—ë´‡ì€ ì‚¬ëŒê³¼ ëŒ€í™”í•˜ëŠ” AI í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤.
AIê°€ ë­ì˜ˆìš”? AIëŠ” ì¸ê³µì§€ëŠ¥ìœ¼ë¡œ ì»´í“¨í„°ê°€ í•™ìŠµí•˜ê³  ìƒê°í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.
ë‚ ì”¨ ì–´ë•Œìš”? ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ë§‘ê³  ì¢‹ìŠµë‹ˆë‹¤!
ê³µë¶€ ì—´ì‹¬íˆ í•˜ì„¸ìš”! ë„¤ ì—´ì‹¬íˆ í•˜ê² ìŠµë‹ˆë‹¤!
ë™ì•„ë¦¬ê°€ ë­ì˜ˆìš”? ë™ì•„ë¦¬ëŠ” ê°™ì€ ê´€ì‹¬ì‚¬ë¥¼ ê°€ì§„ ì‚¬ëŒë“¤ì˜ ëª¨ì„ì…ë‹ˆë‹¤.
í”„ë¡œê·¸ë˜ë°ì´ ë­ì˜ˆìš”? í”„ë¡œê·¸ë˜ë°ì€ ì»´í“¨í„°ì—ê²Œ ëª…ë ¹ì„ ë‚´ë¦¬ëŠ” ê²ƒì…ë‹ˆë‹¤.
íŒŒì´ì¬ì´ ë­ì˜ˆìš”? íŒŒì´ì¬ì€ ì‰½ê³  ê°•ë ¥í•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.
ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”? ì €ëŠ” ì˜ ì§€ë‚´ê³  ìˆìŠµë‹ˆë‹¤!
ê°ì‚¬í•©ë‹ˆë‹¤! ì²œë§Œì—ìš”! ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”.
"""

# ========== ëª¨ë¸ ì´ˆê¸°í™” ==========

tokenizer = SimpleTokenizer()
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# ì‘ì€ ëª¨ë¸ (ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´)
model = GPT3(
    vocab_size=tokenizer.vocab_size,
    block_size=128,
    n_layers=4,
    embed_dim=128,
    n_heads=4
).to(device)

# ê°„ë‹¨í•œ í•™ìŠµ
def train_model():
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)
    data = tokenizer.encode(TRAINING_DATA)
    
    if len(data) < 10:
        return
    
    model.train()
    for epoch in range(100):  # ë¹ ë¥¸ í•™ìŠµ
        for i in range(0, len(data) - 32, 16):
            x = torch.tensor([data[i:i+32]], dtype=torch.long, device=device)
            y = torch.tensor([data[i+1:i+33]], dtype=torch.long, device=device)
            
            logits, loss = model(x, y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        if epoch % 20 == 0:
            print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

print("ëª¨ë¸ í•™ìŠµ ì¤‘...")
train_model()
print("ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

# ========== ì§€ì‹ë² ì´ìŠ¤ (í´ë°±) ==========

KNOWLEDGE = {
    "ì•ˆë…•": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
    "ì´ë¦„": "ì €ëŠ” GPT-3 ê¸°ë°˜ AI ì±—ë´‡ì…ë‹ˆë‹¤!",
    "ë‚ ì”¨": "ì£„ì†¡í•˜ì§€ë§Œ ì‹¤ì‹œê°„ ë‚ ì”¨ ì •ë³´ëŠ” ì œê³µí•  ìˆ˜ ì—†ì–´ìš”.",
    "ì‹œê°„": "í˜„ì¬ ì‹œê°„ì„ í™•ì¸í•´ë³´ì„¸ìš”!",
    "ë„ì›€": "ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! ì œê°€ ìµœì„ ì„ ë‹¤í•´ ë‹µë³€ë“œë¦´ê²Œìš”.",
}

# ========== Flask ì•± ==========

app = Flask(__name__)
CORS(app)

@app.route('/')
def home():
    return '''
    <h1>ğŸ¤– GPT-3 ì±—ë´‡ ì„œë²„</h1>
    <p>ì„œë²„ê°€ ì •ìƒ ì‘ë™ì¤‘ì…ë‹ˆë‹¤!</p>
    <p>POST /chat ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.</p>
    '''

@app.route('/chat', methods=['POST'])
def chat():
    try:
        data = request.get_json()
        message = data.get('message', '')
        
        if not message:
            return jsonify({'response': 'ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!'})
        
        # ì§€ì‹ë² ì´ìŠ¤ í™•ì¸
        for key, value in KNOWLEDGE.items():
            if key in message.lower():
                return jsonify({'response': value})
        
        # GPT ëª¨ë¸ ì‚¬ìš©
        context = f"ì‚¬ìš©ì: {message}\nAI: "
        encoded = tokenizer.encode(context)
        
        if len(encoded) > 0:
            x = torch.tensor([encoded], dtype=torch.long, device=device)
            y = generate(model, x, max_new_tokens=50, temperature=0.8)
            response = tokenizer.decode(y[0].tolist())
            
            # ì‘ë‹µ ì •ë¦¬
            if "AI:" in response:
                response = response.split("AI:")[-1].strip()
                if "ì‚¬ìš©ì:" in response:
                    response = response.split("ì‚¬ìš©ì:")[0].strip()
            
            if len(response) < 5:
                response = "í¥ë¯¸ë¡œìš´ ì§ˆë¬¸ì´ë„¤ìš”! ë” ìì„¸íˆ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?"
        else:
            response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì´í•´í•˜ì§€ ëª»í–ˆì–´ìš”."
        
        return jsonify({'response': response})
    
    except Exception as e:
        print(f"Error: {e}")
        return jsonify({'response': 'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.'})

@app.route('/train', methods=['POST'])
def train():
    """ì‚¬ìš©ìê°€ í•™ìŠµ ë°ì´í„°ë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŒ"""
    try:
        data = request.get_json()
        question = data.get('question', '')
        answer = data.get('answer', '')
        
        if question and answer:
            # ì§€ì‹ë² ì´ìŠ¤ì— ì¶”ê°€
            KNOWLEDGE[question.lower()] = answer
            return jsonify({'success': True, 'message': 'AIê°€ í•™ìŠµí–ˆìŠµë‹ˆë‹¤!'})
        
        return jsonify({'success': False, 'message': 'ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.'})
    
    except Exception as e:
        return jsonify({'success': False, 'message': str(e)})

if __name__ == '__main__':
    print("=" * 50)
    print("ğŸ¤– GPT-3 ì±—ë´‡ ì„œë²„ ì‹œì‘!")
    print("=" * 50)
    print(f"Device: {device}")
    print(f"Vocab Size: {tokenizer.vocab_size}")
    print(f"Model Parameters: {sum(p.numel() for p in model.parameters()):,}")
    print("=" * 50)
    print("ì„œë²„ ì£¼ì†Œ: http://localhost:5000")
    print("=" * 50)
    
    app.run(host='0.0.0.0', port=5000, debug=True)

